\documentclass[a4paper,12pt]{amsart}
\usepackage[utf8]{inputenc}\usepackage[T1]{fontenc}

\title[Multi-Agent Deep Deterministic Policy Gradients]{The Tennis environment solved using a Deep Deterministic Policy Gradient algorithm}
\author{Julian Wergieluk}\address{}\email{julian.wergieluk@risklab.com}
\input{preamble}
\input{commands}

\usepackage[url=false,backend=biber]{biblatex}
\addbibresource{rl.bib}
\addbibresource{ddpg.bib}

\newcommand{\stateSpace}{\mathbb S}
\newcommand{\stateSpaceAlg}{\mathcal S}
\newcommand{\coffinSpace}{\Delta}
\newcommand{\actionSpace}{\mathbb A}
\newcommand{\actionSpaceAlg}{\mathcal A}
\newcommand{\stateValueFunc}{V}
\newcommand{\actionValueFunc}{Q}
\newcommand{\advantageFunc}{A}
\newcommand{\policy}{\pi}
\newcommand{\policyLik}{f}
\newcommand{\discountFactor}{\gamma}
\newcommand{\prob}{\mathbb P}
\newcommand{\rewardFunc}{\phi}
\newcommand{\trajectory}{\tau}
\newcommand{\trajectorySpace}{\mathbb T}
\newcommand{\startStateDist}{\rho_0}

\begin{document}

\maketitle

\begin{abstract}
This short note provides a concise description of the model architecture and
learning algorithms of the agent developed in this project. We also report learning
performance of the agent and provide a list of possible future model improvements.
\end{abstract}

\section{Description of the learning algorithm}

For this problem, we use a standard Deep Deterministic Policy Gradient 
algorithm (DDPG) and parameterize it in a way to make it suitable for multi-agent
environments.

Deep deterministic policy gradient algorithm is an actor-critic type method
with a deterministic actor policy $\pi$ mapping states from $\stateSpace$ to
actions in $\actionSpace$. During the training, the action-value
function $Q$ is optimized using a standard DQN algorithm as described in
\cite{mnih2015humanlevel} employing a replay buffer and a separate target
network.

The policy network is trained using a policy gradient derived from the action-value
function approximation. Specifically, the parameter vector $\theta$ is given as
$\theta = (\theta^{Q}, \theta^{\pi})$, where $\theta^{Q}$ determines $Q$ and
$\theta^{\pi}$ determines $\pi$. Let $J(\theta)$ be the cumulative expected 
reward. Then the policy gradient can be approximated by
\begin{align*}
    \nabla_{\theta^{\pi}} J & \approx
    \E \left[ \nabla_{a} Q(S_t, \pi(S_t)) \nabla_{\theta^\pi} \pi(S_t) \right].
\end{align*}

To improve the algorithm stability we use soft target updates for the
parameters of both actor and critic networks, described in
\cite{mnih2015humanlevel}, and batch normalization.

\section{Training analysis}

We train a DDPG agent for 1900 episodes consisting of at most 1000 time steps
each. The cumulative reward averaged over 100 episodes reaches the level of
0.5017 after completing the episode 494. 

\begin{figure}[tb]
    \centering
    \includegraphics[width=\textwidth]{{scores}.pdf}
    \caption{We train a DDPG agent for 1900 episodes. The ``reward'' curve gives
        the maximum of individual agents rewards at the end of each episode. The
        ``average reward'' curve is the rolling mean of the ``reward'' curve
        calculated over 100 episodes. (For episodes $1,\cdots, 99$ we use an
        expanding window.)}
    \label{fig:scores}
\end{figure}

\section{Algorithm parametrization}

\paragraph{The Actor network architecture.} The actor is a feed-forward neural
network mapping the state of the environment to a deterministic action. The
network consists of three layers sandwiched with batch normalization. The final
layer uses the tanh nonlinearity to produce an action vector in the cube $[-1, 1]^{2}$.

\verbatiminput{actor-net.txt}

\paragraph{The Critic network architecture.} The critic is a feed-forward neural 
network approximating the state-action value function $Q$ mapping state-action
pairs to the expected cumulative reward. The critic network consists of two subnetworks: 
a one-layer network combined with batch normalization that maps the state to a high-dimensional 
state representation, and a two-layer network that maps the aforementioned representation
of the state concatenated with the action vector to the approximation of the state-action
value $Q(s,a)$.

\verbatiminput{critic-net.txt}

The list of hyperparameters used by the agent is listed in Table \ref{tab:hyperparameters}.

\begin{table}
%\centering
\caption{List of hyperparameters and their values}
\begin{tabular}{|l|l|l|l|}
    \hline
Hyperparameter & Variable name & Value \\ 
    \hline \hline
    Replay buffer size & \texttt{BUFFER\_SIZE} &  1e6 \\ \hline
    Batch size & \texttt{BATCH\_SIZE} &  128  \\  \hline
    Discount factor ($\gamma$) & \texttt{GAMMA} &  0.99 \\ \hline
    Tau & \texttt{TAU} & 1e-3 \\ \hline
    Actor learning rate & \texttt{LR\_ACTOR} & 0.001 \\ \hline
    Critic learning rate & \texttt{LR\_CRITIC} & 0.001 \\ \hline
    Initial value of the & \texttt{EPSILON} & 1.0 \\
    OU process scaling factor $\varepsilon$ && \\ \hline
    OU mean reversion level & & 0.0 \\ \hline
    OU mean reversion speed & & 0.15 \\ \hline
    OU volatility (sigma) & & 0.2 \\ \hline
    Learning frequency & \texttt{} &  every 5 time steps \\ \hline
\end{tabular}
\label{tab:hyperparameters}
\end{table}

\section{Ideas for future work}

\begin{itemize}
    \item Use of a real multi-agent reinforcement learning algorithm explicitly taking the
        non-stationarity of the environment into the account.
    \item Actor and critic could use a common network for state processing. 
\end{itemize}

\nocite{lillicrap2015continuous}
\printbibliography

\end{document}

% vim: spelllang=en_us:spell:
